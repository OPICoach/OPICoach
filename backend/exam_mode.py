# exam_mode.py
import os
from openai import AsyncOpenAI
from utils import postprocess_output, load_prompt_template
from db_utils.chroma_db_utils import retrieve_random_context, retrieve_related_context
from db_utils.user_db_utils import get_user_info  # ìœ ì € ì •ë³´ ì¡°íšŒ í•¨ìˆ˜ ì„í¬íŠ¸

# Gemini(OpenAI í˜¸í™˜ API) í´ë¼ì´ì–¸íŠ¸
openai_client = AsyncOpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)
model_name = "gemini-2.5-pro-preview-05-06"
PROMPT_MAKEEXAM_FILE = "./prompt/prompt_makeexam.txt"
PROMPT_FEEDBACKS_FILE = "./prompt/prompt_feedbacks.txt"


async def ask_llm(question, answer, context, prompt_template, history=None):
    prompt = prompt_template.format(context=context, question=question, answer=answer)
    messages = []
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": prompt})

    try:
        response = await openai_client.chat.completions.create(
            model=model_name,
            messages=messages,
        )
        raw_content = response.choices[0].message.content
        raw_output = raw_content.strip() if raw_content else ""
        return postprocess_output(raw_output)
    except Exception as e:
        print(f"\033[1;31mğŸ”¥ ask_llm ì˜¤ë¥˜:\033[0m {e}")
        return ""

# ì‹œí—˜ ë¬¸ì œ ìƒì„± í•¨ìˆ˜ (user_pk ë°›ì•„ì„œ ìœ ì € ì •ë³´ í¬í•¨)
async def generate_exam_question(user_pk: int):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        user_info = get_user_info(user_pk)  # DBì—ì„œ ìœ ì € ì •ë³´ ì¡°íšŒ (dictí˜•íƒœë¼ê³  ê°€ì •)
        prompt_makeexam_template = await load_prompt_template(PROMPT_MAKEEXAM_FILE)
        context_question = retrieve_random_context("exam_docs", n_results=10)

        # í”„ë¡¬í”„íŠ¸ì— user_infoë¥¼ ì¶”ê°€í•´ì„œ í¬ë§· (ì˜ˆ: user_info ë”•ì…”ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
        prompt_with_user = prompt_makeexam_template.format(
            user_info=user_info_to_str(user_info),
            context=context_question,
            question="",
            answer=""
        )
        question = await ask_llm("", "", context_question, prompt_with_user)
        print(f"\033[1;34mâ“ ìƒì„±ëœ ì§ˆë¬¸:\033[0m {question}")
        return question
    except Exception as e:
        print(f"\033[1;31mğŸ”¥ generate_exam_question ì˜¤ë¥˜:\033[0m {e}")
        raise

# ì‹œí—˜ í”¼ë“œë°± ìƒì„± í•¨ìˆ˜ (user_pk ë°›ì•„ì„œ ìœ ì € ì •ë³´ í¬í•¨)
async def generate_exam_feedback(user_pk: int, question: str, user_answer: str):
    """ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì‚¬ìš©ìì˜ ë‹µë³€ì— ëŒ€í•œ í”¼ë“œë°±ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        user_info = get_user_info(user_pk)
        prompt_feedbacks_template = await load_prompt_template(PROMPT_FEEDBACKS_FILE)
        context_feedback = retrieve_related_context(question, user_answer, "exam_docs", n_results=10)

        prompt_with_user = prompt_feedbacks_template.format(
            user_info=user_info_to_str(user_info),
            context=context_feedback,
            question=question,
            answer=user_answer
        )
        feedback = await ask_llm(question, user_answer, context_feedback, prompt_with_user)
        print(f"\n\033[1;33mğŸ”§ ìƒì„±ëœ í”¼ë“œë°±:\033[0m\n{feedback}")
        return feedback
    except Exception as e:
        print(f"\033[1;31mğŸ”¥ generate_exam_feedback ì˜¤ë¥˜:\033[0m {e}")
        raise

def user_info_to_str(user_info):
    """
    user_info ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ ë³€í™˜.
    í•„ìš”í•œ í•„ë“œë§Œ í¬í•¨í•˜ê±°ë‚˜ í¬ë§· ì¡°ì • ê°€ëŠ¥.
    """
    if not user_info:
        return "User information not available."
    # ì˜ˆì‹œ: ì£¼ìš” ì •ë³´ë§Œ ë½‘ì•„ì„œ ë¬¸ìì—´ ìƒì„±
    fields = ["past_opic_level", "goal_opic_level", "background", "occupation_or_major", "topics_of_interest"]
    info_str = ", ".join(f"{field}: {user_info.get(field, 'N/A')}" for field in fields)
    return info_str
