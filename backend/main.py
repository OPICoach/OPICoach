import os
import pyaudio
import wave
from google.cloud import speech
from google.cloud import texttospeech
import threading
import asyncio
from sentence_transformers import SentenceTransformer
import chromadb
from openai import AsyncOpenAI
import time
import pygame
import random

from dotenv import load_dotenv

# Google Cloud Speech-to-Text ì„¤ì •
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "./opicoach-e1cb367c0382.json"

# ê²½ë¡œ ì„¤ì •
CHROMA_DB_DIR = "./chroma_db"
TIPS_COLLECTION_NAME = "tips_docs"
FEEDBACKS_COLLECTION_NAME = "feedbacks_docs"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
PROMPT_FILE = "./prompt/prompt_test.txt"
PROMPT_MAKEEXAM_FILE = "./prompt/prompt_makeexam.txt"
PROMPT_FEEDBACKS_FILE = "./prompt/prompt_feedbacks.txt"

# ëª¨ë¸ ë° DB ì´ˆê¸°í™”
embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)
tips_collection = chroma_client.get_or_create_collection(name=TIPS_COLLECTION_NAME)
feedbacks_collection = chroma_client.get_or_create_collection(name=FEEDBACKS_COLLECTION_NAME)

# Gemini(OpenAI í˜¸í™˜ API) í´ë¼ì´ì–¸íŠ¸
openai_client = AsyncOpenAI(
    api_key=os.environ.get('API_KEY'),
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)
model_name = "gemini-2.5-pro-preview-05-06"

# Google Cloud Text-to-Speech í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
tts_client = texttospeech.TextToSpeechClient()

# í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
def load_prompt_template(file_path):
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        return "Answer the question based on the following context:\n\n{context}\n\nQuestion: {question}"

# ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
def retrieve_context(query, collection, top_k=3):
    query_embedding = embed_model.encode([query])[0].tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)
    documents = results.get("documents", [])
    if documents:
        flat_docs = [doc for sublist in documents for doc in sublist if isinstance(doc, str)]
        return "\n\n".join(flat_docs)
    else:
        return ""

# ëœë¤ ë¬¸ì„œ ê²€ìƒ‰ (í”¼ë“œë°± DBì—ì„œ)
def retrieve_random_context(collection, n_results=10):
    all_ids = collection.get(include=[])['ids']
    if not all_ids:
        return ""
    random_ids = random.sample(all_ids, min(n_results, len(all_ids)))
    results = collection.get(ids=random_ids, include=["documents"])
    documents = results.get("documents", [])
    if documents:
        flat_docs = [doc for sublist in documents for doc in sublist if isinstance(doc, str)]
        return "\n\n".join(flat_docs)
    else:
        return ""

# íŠ¹ì • ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (í”¼ë“œë°± DBì—ì„œ)
def retrieve_related_context(collection, question, answer, n_results=10):
    combined_query = f"Question: {question}\nAnswer: {answer}"
    query_embedding = embed_model.encode([combined_query])[0].tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=n_results)
    documents = results.get("documents", [])
    if documents:
        flat_docs = [doc for sublist in documents for doc in sublist if isinstance(doc, str)]
        return "\n\n".join(flat_docs)
    else:
        return ""

# ì¶œë ¥ í›„ì²˜ë¦¬ (ì´ëª¨í‹°ì½˜ ì œê±°)
def postprocess_output(text):
    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace() or char in ['.', ',', '?', '!'])
    return cleaned_text.replace("*", "").strip()

# LLM í˜¸ì¶œ
async def ask_llm(question, answer, context, prompt_template, history=None):
    prompt = prompt_template.format(context=context, question=question, answer=answer)
    messages = []
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": prompt})

    response = await openai_client.chat.completions.create(
        model=model_name,
        messages=messages,
    )
    raw_output = response.choices[0].message.content.strip()
    return postprocess_output(raw_output)

# ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ (ì‚¬ìš©ì ì…ë ¥ ì¢…ë£Œ ì‹œê°„ ì ìš©)
def recognize_speech_from_microphone(timeout=5):
    client = speech.SpeechClient()
    p = pyaudio.PyAudio()
    stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)
    print(f"\nğŸ¤ \033[1;32mìŒì„±ì„ {timeout}ì´ˆ ë™ì•ˆ ë§í•´ì£¼ì„¸ìš”...\033[0m")  # ë…¹ìƒ‰ ê°•ì¡°
    audio_data = b""  # ìŒì„± ë°ì´í„° ì´ˆê¸°í™”
    start_time = time.time()
    while True:
        audio_data += stream.read(1024)
        if time.time() - start_time > timeout:
            print(f"â±ï¸ \033[1;33m{timeout}ì´ˆê°€ ê²½ê³¼í–ˆìŠµë‹ˆë‹¤. ìŒì„±ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\033[0m")  # ë…¸ë€ìƒ‰ ê°•ì¡°
            break
    audio = speech.RecognitionAudio(content=audio_data)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="ko-KR",
        alternative_language_codes=["en-US"],
        model="latest_short",
        use_enhanced=True,
        speech_contexts=[speech.SpeechContext(phrases=["ì˜¤í”½", "opic", "ì˜¤í”½ ì‹œí—˜", "opic test"], boost=20.0)]
    )
    try:
        response = client.recognize(config=config, audio=audio)
        if response.results:
            transcript = response.results[0].alternatives[0].transcript
            print(f"ğŸ“ \033[1;34mì¸ì‹ëœ ìŒì„±:\033[0m {transcript}")  # íŒŒë€ìƒ‰ ê°•ì¡°
            return transcript
        else:
            print("ğŸ”‡ \033[1;31mìŒì„±ì´ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\033[0m")  # ë¹¨ê°„ìƒ‰ ê°•ì¡°
            return ""
    except Exception as e:
        print(f"âš ï¸ \033[1;31mìŒì„± ì¸ì‹ ì˜¤ë¥˜:\033[0m {e}")  # ë¹¨ê°„ìƒ‰ ê°•ì¡°
        return ""

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¬ìƒí•˜ëŠ” í•¨ìˆ˜
def speak_response(text, voice_name="ko-KR-Chirp3-HD-Leda"):
    text = text.lower().replace("opic", "ì˜¤í”½").replace("OPIc", "ì˜¤í”½").replace("topic", "í† í”½")

    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(language_code="ko-KR", name=voice_name)
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

    try:
        response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)
        with open("output.mp3", "wb") as out:
            out.write(response.audio_content)
            print(f"ğŸ”Š ìŒì„± íŒŒì¼ ì €ì¥ ì™„ë£Œ: output.mp3 (í™”ì: {voice_name})")
        pygame.mixer.init()
        pygame.mixer.music.load("output.mp3")
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            pygame.time.Clock().tick(10)
        pygame.mixer.quit()
        os.remove("output.mp3")
        print("ğŸ—‘ï¸ ì„ì‹œ ìŒì„± íŒŒì¼ ì œê±° ì™„ë£Œ.")
    except Exception as e:
        print(f"âš ï¸ TTS ì˜¤ë¥˜: {e}")

# ì‚¬ìš©ì ì„ íƒ ì²˜ë¦¬
def get_input_method():
    while True:
        print("\n\033[1;36mğŸ’¬ ë‹µë³€ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:\033[0m")  # ì²­ë¡ìƒ‰ ê°•ì¡°
        choice = input("1. í…ìŠ¤íŠ¸ë¡œ ë‹µë³€\n2. ìŒì„±ìœ¼ë¡œ ë‹µë³€\n(\033[1;36mí…ìŠ¤íŠ¸/ìŒì„±\033[0m): ").strip().lower()  # ì²­ë¡ìƒ‰ ê°•ì¡°
        if choice == "í…ìŠ¤íŠ¸":
            return "í…ìŠ¤íŠ¸"
        elif choice == "ìŒì„±":
            return "ìŒì„±"
        else:
            print("âš ï¸ \033[1;31mì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 'í…ìŠ¤íŠ¸' ë˜ëŠ” 'ìŒì„±'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\033[0m")  # ë¹¨ê°„ìƒ‰ ê°•ì¡°

def get_mode_choice():
    while True:
        print("\n\033[1;36mğŸ’¬ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\033[0m")  # ì²­ë¡ìƒ‰ ê°•ì¡°
        choice = input("1. í•™ìŠµ ëª¨ë“œ\n2. ì‹œí—˜ ëª¨ë“œ\n(\033[1;36mí•™ìŠµ/ì‹œí—˜\033[0m): ").strip().lower()  # ì²­ë¡ìƒ‰ ê°•ì¡°
        if choice == "í•™ìŠµ":
            return "í•™ìŠµ"
        elif choice == "ì‹œí—˜":
            return "ì‹œí—˜"
        else:
            print("âš ï¸ \033[1;31mì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 'í•™ìŠµ' ë˜ëŠ” 'ì‹œí—˜'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\033[0m")  # ë¹¨ê°„ìƒ‰ ê°•ì¡°

# ì‚¬ìš©ì ì„ íƒ ì²˜ë¦¬: ìŒì„± ì¶œë ¥ ì—¬ë¶€ ì¶”ê°€ (Y/N)
def get_audio_output_choice():
    while True:
        choice = input("\n\033[1;36mğŸ’¬ ìŒì„± í”¼ë“œë°±ì„ ë“¤ì„ê¹Œìš”? (Y/N): \033[0m").strip().upper()
        
        if choice == "Y":
            return True
        elif choice == "N":
            return False
        else:
            print("âš ï¸ \033[1;31mì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 'Y' ë˜ëŠ” 'N'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\033[0m")  # ë¹¨ê°„ìƒ‰ ê°•ì¡°

async def exam_mode():
    prompt_makeexam_template = load_prompt_template(PROMPT_MAKEEXAM_FILE)
    prompt_feedbacks_template = load_prompt_template(PROMPT_FEEDBACKS_FILE)
    exam_history = []
    question_number = 0

    while True:
        question_number += 1
        print(f"\n\033[1;33mğŸ“ ì‹œí—˜ ë¬¸ì œ {question_number}\033[0m") # ë…¸ë€ìƒ‰ ê°•ì¡°
        context_question = retrieve_random_context(feedbacks_collection, n_results=10)
        question = await ask_llm("", "", context_question, prompt_makeexam_template)
        print(f"\033[1;34mâ“ ì§ˆë¬¸:\033[0m {question}") # íŒŒë€ìƒ‰ ê°•ì¡°
        print("\n\033[1;32mâ³ 3ì´ˆ í›„ ë‹µë³€ì„ ì‹œì‘í•˜ì„¸ìš”...\033[0m") # ë…¹ìƒ‰ ê°•ì¡°
        time.sleep(3)

        input_method = get_input_method()
        user_answer = ""
        if input_method == "í…ìŠ¤íŠ¸":
            user_answer = input("\n\033[1;36mğŸ’¬ ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (30ì´ˆ ì œí•œ): \033[0m") # ì²­ë¡ìƒ‰ ê°•ì¡°
            # ì‹¤ì œ ì‹œí—˜ì—ì„œëŠ” 30ì´ˆ ì œí•œ ë¡œì§ í•„ìš”
        elif input_method == "ìŒì„±":
            print("\n\033[1;36mğŸ’¬ 30ì´ˆ ë™ì•ˆ ë‹µë³€í•´ì£¼ì„¸ìš”...\033[0m") # ì²­ë¡ìƒ‰ ê°•ì¡°
            user_answer = recognize_speech_from_microphone(timeout=30)
            # ì‹¤ì œ ì‹œí—˜ì—ì„œëŠ” 30ì´ˆ ì œí•œ ë¡œì§ í•„ìš”

        print(f"\n\033[1;35mğŸ—£ï¸ ë‹¹ì‹ ì˜ ë‹µë³€:\033[0m {user_answer}") # ë³´ë¼ìƒ‰ ê°•ì¡°

        context_feedback = retrieve_related_context(feedbacks_collection, question, user_answer, n_results=10)
        feedback = await ask_llm(question, user_answer, context_feedback, prompt_feedbacks_template)
        print(f"\n\033[1;33mğŸ”§ í”¼ë“œë°±:\033[0m\n{feedback}") # ë…¸ë€ìƒ‰ ê°•ì¡°
        if get_audio_output_choice():
            speak_response(feedback)

        exam_history.append({
            "question": question,
            "answer": user_answer,
            "feedback": feedback,
            "number": question_number
        })

        while(True):
            continue_exam = input("\n\033[1;36mê³„ì†í•´ì„œ ì‹œí—˜ì„ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (Y/N): \033[0m").strip().upper()

            if continue_exam == "Y":
                break
            
            elif continue_exam == "N":
                # ì‹œí—˜ ì¢…ë£Œ ì‹œ ê° ë¬¸ì œë³„ ê²°ê³¼ ì €ì¥
                if exam_history:
                    print("\n\033[1;32mğŸ’¾ ê° ì‹œí—˜ ë¬¸ì œë³„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...\033[0m") # ë…¹ìƒ‰ ê°•ì¡°

                    save_dir = "test_result"
                    os.makedirs(save_dir, exist_ok=True)

                    for entry in exam_history:
                        filename = f"opic_question_{entry['number']}_{time.strftime('%Y%m%d_%H%M%S')}.txt"
                        filepath = os.path.join(save_dir, filename)

                        with open(filepath, "w", encoding="utf-8") as f:
                            f.write(f"--- ë¬¸ì œ {entry['number']} ---\n")
                            f.write(f"ì§ˆë¬¸: {entry['question']}\n")
                            f.write(f"ë‹µë³€: {entry['answer']}\n")
                            f.write(f"í”¼ë“œë°±:\n{entry['feedback']}\n")
                        print(f"ğŸ“„ ë¬¸ì œ {entry['number']} ê²°ê³¼ê°€ '{filename}'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    print("\n\033[1;32mğŸ‰ ëª¨ë“  ì‹œí—˜ ê²°ê³¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\033[0m") # ë…¹ìƒ‰ ê°•ì¡°
                else:
                    print("\n\033[1;33mâš ï¸ ì§„í–‰ëœ ì‹œí—˜ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.\033[0m") # ë…¸ë€ìƒ‰ ê°•ì¡°
                return
            else:
                print("âš ï¸ \033[1;31mì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 'Y' ë˜ëŠ” 'N'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\033[0m") # ë¹¨ê°„ìƒ‰ ê°•ì¡°

async def learning_mode():
    prompt_template = load_prompt_template(PROMPT_FILE)
    chat_history = []
    while True:
        input_method = get_input_method()
        question = ""
        if input_method == "í…ìŠ¤íŠ¸":
            question = input("\n\033[1;34mâ“ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'end' ì…ë ¥):\033[0m ")  # íŒŒë€ìƒ‰ ê°•ì¡°
        elif input_method == "ìŒì„±":
            timeout = int(input("\n\033[1;33mâ° ìŒì„± ì¸ì‹ì„ ëª‡ ì´ˆ ë™ì•ˆ í• ê¹Œìš”? (ì´ˆ ë‹¨ìœ„ ì…ë ¥):\033[0m "))  # ë…¸ë€ìƒ‰ ê°•ì¡°
            question = recognize_speech_from_microphone(timeout=timeout)

        if question.lower().strip() == "end":
            print("\n\033[1;31mğŸ›‘ ì¢…ë£Œí•©ë‹ˆë‹¤.\033[0m")  # ë¹¨ê°„ìƒ‰ ê°•ì¡°
            break

        try:
            context = retrieve_context(question, tips_collection)
            answer = await ask_llm(question, "", context, prompt_template, chat_history)
            print(f"\n\033[1;35mğŸ§  ë‹µë³€:\033[0m\n{answer}")  # ë³´ë¼ìƒ‰ ê°•ì¡°

            # ìŒì„± ì¶œë ¥ ì—¬ë¶€ í™•ì¸
            if get_audio_output_choice():
                speak_response(answer)

            chat_history.append({"role": "user", "content": question})
            chat_history.append({"role": "assistant", "content": answer})

            print("\n\033[1;30m--- ëŒ€í™” ê¸°ë¡ ---\033[0m")  # íšŒìƒ‰ ê°•ì¡°
            for turn in chat_history:
                role = turn["role"]
                content = turn["content"]
                if role == "user":
                    print(f"\033[1;34mì‚¬ìš©ì:\033[0m {content}")  # íŒŒë€ìƒ‰ ê°•ì¡°
                elif role == "assistant":
                    print(f"\033[1;35mOPICoach:\033[0m {content}")  # ë³´ë¼ìƒ‰ ê°•ì¡°
                    print()
            print("\033[1;30m-------------------\033[0m")  # íšŒìƒ‰ ê°•ì¡°

        except Exception as e:
            print(f"\n\033[1;31mâš ï¸ ì˜¤ë¥˜ ë°œìƒ:\033[0m {e}")  # ë¹¨ê°„ìƒ‰ ê°•ì¡°

# ë©”ì¸ í•¨ìˆ˜
async def main():
    pygame.init()
    print("\n\033[1;32mğŸ‰ OPICoachì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\033[0m")  # ë…¹ìƒ‰ ê°•ì¡°

    mode = get_mode_choice()

    if mode == "í•™ìŠµ":
        await learning_mode()
    elif mode == "ì‹œí—˜":
        await exam_mode()

if __name__ == "__main__":
    asyncio.run(main())