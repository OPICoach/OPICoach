import os
from utils import load_prompt_template, ask_llm
from db_utils.chatlog_db_utils import load_chat_history
from db_utils.learning_note_db_utils import save_learning_note, get_learning_notes, delete_learning_note
from typing import List, Optional, Dict


model_name = "gemini-2.5-pro-preview-05-06"
LEARNING_NOTE_PROMPT_FILE = "./prompt/prompt_learning_note.txt"


async def generate_learning_note(*, user_pk: int, session_pk: int, title: str = None, LLM_model: str = "gemini-2.0-flash") -> Optional[int]:
    """ì±„íŒ… ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ë…¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        print(f"\nğŸ¤– ì„ íƒëœ AI ëª¨ë¸: {LLM_model}")
        
        # ì±„íŒ… ê¸°ë¡ ë¡œë“œ
        if callable(getattr(load_chat_history, "__await__", None)):
            chat_history = await load_chat_history(user_pk, session_pk)
        else:
            chat_history = load_chat_history(user_pk, session_pk)

        if not chat_history:
            return None

        # ì±„íŒ… ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        chat_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        prompt_template = await load_prompt_template(LEARNING_NOTE_PROMPT_FILE)

        # LLMì„ í†µí•´ ë…¸íŠ¸ ìƒì„±
        note_content = await ask_llm("", "", chat_text, prompt_template, __model_name=LLM_model)
        
        if not note_content or note_content == "ì±„íŒ… ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.":
            return None

        # ë…¸íŠ¸ ì €ì¥
        note_pk = save_learning_note(user_pk, title, note_content, session_pk)
        return note_pk

    except Exception as e:
        print(f"\nâš ï¸ í•™ìŠµ ë…¸íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return None 