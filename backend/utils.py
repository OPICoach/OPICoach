# utils.py
import os
import random
import asyncio
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

# ì¶œë ¥ í›„ì²˜ë¦¬ (ì´ëª¨í‹°ì½˜ ì œê±°)
def postprocess_output(text):
    # if not isinstance(text, str):
    #     return ""  # ì…ë ¥ì´ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    # cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace() or char in ['.', ',', '?', '!'])
    # return cleaned_text.replace("*", "").strip()

    # \nê³¼ \\nì„ <br>ë¡œ ë³€í™˜
    text = text.replace("\\n", "<br>").replace("\n", "<br>")

    return text if text else ""

# í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (ë¹„ë™ê¸°)
async def load_prompt_template(file_path, user_info: dict = None):
    try:
        loop = asyncio.get_running_loop()
        content = await loop.run_in_executor(None, _read_file, file_path)
        if user_info:
            user_info_str = "\n".join(f"{key}: {value}" for key, value in user_info.items())
            content = content.replace("{user_info}", user_info_str)
        return content
    except FileNotFoundError:
        print(f"\033[1;31mğŸ”¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\033[0m")
        raise
    except Exception as e:
        print(f"\033[1;31mğŸ”¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼({file_path}) ë¡œë”© ì˜¤ë¥˜:\033[0m {e}")
        raise

def _read_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()


# # OpenAI API ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE_URL = os.getenv("OPENAI_API_BASE_URL", "https://generativelanguage.googleapis.com/v1beta/")

# openai_client = AsyncOpenAI(
#     api_key=OPENAI_API_KEY,
#     base_url=OPENAI_API_BASE_URL
# )
# Gemini(OpenAI í˜¸í™˜ API) í´ë¼ì´ì–¸íŠ¸
openai_client = AsyncOpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)

async def ask_llm(question, answer, context, prompt_template, history=None, __model_name="gemini-2.0-flash"):
    prompt = prompt_template.format(context=context, question=question, answer=answer)
    messages = []
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": prompt})

    try:
        response = await openai_client.chat.completions.create(
            model=__model_name,
            messages=messages,
        )

        raw_content = response.choices[0].message.content
        return postprocess_output(raw_content)
    except Exception as e:
        print(f"\033[1;31mğŸ”¥ ask_llm ì˜¤ë¥˜:\033[0m {e}")
        return ""