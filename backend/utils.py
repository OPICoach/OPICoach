# utils.py
import os
import random
import asyncio
import json
from openai import AsyncOpenAI
from pydantic import BaseModel, Field, ValidationError
from dotenv import load_dotenv
load_dotenv()

# ì¶œë ¥ í›„ì²˜ë¦¬ (ì´ëª¨í‹°ì½˜ ì œê±°)
def postprocess_output(text):
    # if not isinstance(text, str):
    #     return ""  # ì…ë ¥ì´ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    # cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace() or char in ['.', ',', '?', '!'])
    # return cleaned_text.replace("*", "").strip()

    # \nê³¼ \\nì„ <br>ë¡œ ë³€í™˜
    text = text.replace("\\n", "<br>").replace("\n", "<br>")

    return text if text else ""

# í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (ë¹„ë™ê¸°)
async def load_prompt_template(file_path, user_info: dict = None):
    try:
        loop = asyncio.get_running_loop()
        content = await loop.run_in_executor(None, _read_file, file_path)
        if user_info:
            user_info_str = "\n".join(f"{key}: {value}" for key, value in user_info.items())
            content = content.replace("{user_info}", user_info_str)
        return content
    except FileNotFoundError:
        print(f"\033[1;31mğŸ”¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\033[0m")
        raise
    except Exception as e:
        print(f"\033[1;31mğŸ”¥ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼({file_path}) ë¡œë”© ì˜¤ë¥˜:\033[0m {e}")
        raise

def _read_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()


# # OpenAI API ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE_URL = os.getenv("OPENAI_API_BASE_URL", "https://generativelanguage.googleapis.com/v1beta/")

# openai_client = AsyncOpenAI(
#     api_key=OPENAI_API_KEY,
#     base_url=OPENAI_API_BASE_URL
# )
# Gemini(OpenAI í˜¸í™˜ API) í´ë¼ì´ì–¸íŠ¸
openai_client = AsyncOpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url="https://generativelanguage.googleapis.com/v1beta/"
)

async def ask_llm(question, answer, context, prompt_template, history=None, __model_name="gemini-2.0-flash"):
    prompt = prompt_template.format(context=context, question=question, answer=answer)
    messages = []
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": prompt})

    try:
        response = await openai_client.chat.completions.create(
            model=__model_name,
            messages=messages,
        )

        raw_content = response.choices[0].message.content
        return postprocess_output(raw_content)
    except Exception as e:
        print(f"\033[1;31mğŸ”¥ ask_llm ì˜¤ë¥˜:\033[0m {e}")
        return ""

# ì‹œí—˜ ìœ í˜• ì •ì˜
exam_types = ["self_introduction", "description", "routine", "comparison", "past_experience", "role_play", "advanced_qustion"]
"""
exam_types_score = {
    "self_introduction": (-3, 1),
    "description": (-2, 2),
    "routine": (-2, 2),
    "comparison": (-2, 2),
    "past_experience": (-2, 3),
    "role_play": (-2, 4),
    "advanced_qustion": (-1, 5),
    "default": (-5, 5)
}
"""
exam_types_score = {
    "self_introduction": (30, 100),
    "description": (20, 20),
    "routine": (20, 30),
    "comparison": (20, 30),
    "past_experience": (20, 30),
    "role_play": (20, 40),
    "advanced_qustion": (20, 50),
    "default": (50, 70)
}


class ScoreResponseModel(BaseModel):
    score: int = Field(ge=20, le=100, description="í”¼ë“œë°±ì— ëŒ€í•œ ì ìˆ˜, -5ì—ì„œ 5 ì‚¬ì´ì˜ ì •ìˆ˜")

async def feedback_to_score(feedback, exam_type="default", __model_name="gemini-2.0-flash"):
    min_score, max_score = exam_types_score.get(exam_type, (20, 100))
    prompt = (
        f"ë‹¤ìŒ í”¼ë“œë°±ì— ëŒ€í•œ ì ìˆ˜ë¥¼ {min_score}ì—ì„œ {max_score} ì‚¬ì´ì˜ ì •ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”. "
        f"ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, 'score'ë¼ëŠ” í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: {{\"score\": 3}}\n\n"
        f"í”¼ë“œë°±: {feedback}\n\n"
        f"JSON ì‘ë‹µ:"
    )
    messages = []
    messages.append({"role": "user", "content": prompt})

    try:
        response = await openai_client.chat.completions.create(
            model=__model_name,
            messages=messages,
            response_format={"type": "json_object"} # OpenAI JSON ëª¨ë“œ ì‚¬ìš©
        )
        
        if response.choices and response.choices[0].message and response.choices[0].message.content:
            raw_content = response.choices[0].message.content
            try:
                # Pydantic ëª¨ë¸ë¡œ ë°ì´í„° ê²€ì¦ ë° íŒŒì‹±
                validated_data = ScoreResponseModel.model_validate_json(raw_content)
                return validated_data.score
            except ValidationError as ve: # JSON íŒŒì‹± ì˜¤ë¥˜ ë° ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜ ì²˜ë¦¬
                print(f"\033[1;31mğŸ”¥ feedback_to_score: ë°ì´í„° ê²€ì¦/íŒŒì‹± ì˜¤ë¥˜: {ve}\033[0m")
                print(f"ì›ë³¸ ë‚´ìš©: {raw_content}")
                return 0
        else:
            print(f"\033[1;33mâš ï¸ feedback_to_score: LLMìœ¼ë¡œë¶€í„° ìœ íš¨í•œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\033[0m")
            return 0
    except Exception as e_api: # API í˜¸ì¶œ ìì²´ì˜ ì˜¤ë¥˜ ë“± ê¸°íƒ€ ì˜ˆì™¸
        print(f"\033[1;31mğŸ”¥ feedback_to_score API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e_api}\033[0m")
        return 0

if __name__ == "__main__":
    import asyncio

    # ask_llm_score í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    feedback_to_test = "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í”¼ë“œë°±ì…ë‹ˆë‹¤. ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”."
    print(f"í…ŒìŠ¤íŠ¸ í”¼ë“œë°±: {feedback_to_test}")
    
    # ask_llm_score í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ
    score_result = asyncio.run(feedback_to_score(feedback_to_test))
    print(f"LLM í‰ê°€ ì ìˆ˜: {score_result}")